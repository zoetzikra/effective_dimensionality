{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f4c761-4696-439e-8b40-722e0540ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchattacks\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hess import data\n",
    "import hess.nets as models\n",
    "from model import make_cnn, accuracy\n",
    "from hess_vec_prod import min_max_hessian_eigs\n",
    "\n",
    "from utils import accuracy\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "def eff_dim(x, s = 1.):\n",
    "    x = x[x!=1.] #remove eigenvalues that didnt converge from the lanczos computation to make things less noisy\n",
    "    return np.sum(x / (x + s))\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "print(f\"CUDA: {CUDA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c006c73-43e4-4ba8-b221-747e121d5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/bonlime/4e0d236cf98cd5b15d977dfa03a63643"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619a787-dbdf-412f-82c6-a526ccea12d1",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c97c837-0a04-4069-a173-4c2e208002c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 1000\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root='../imagenet/val', transform=preprocess)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=8)\n",
    "\n",
    "trainset =  torchvision.datasets.ImageFolder(root='../imagenet/train', transform=preprocess)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=8)\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, topk=(1,), log=False):\n",
    "    all_output = None\n",
    "    all_true_labels = None\n",
    "    first = True\n",
    "\n",
    "    model.eval()\n",
    "    for x, y in dataloader:\n",
    "      inputs = x.to(device)\n",
    "\n",
    "      logits = model(inputs)\n",
    "      model.zero_grad()\n",
    "\n",
    "      if first:\n",
    "          all_output = logits.detach().cpu()\n",
    "          all_true_labels = y.detach()\n",
    "          first = False\n",
    "      else:\n",
    "          all_output = torch.concat([all_output, logits.detach().cpu()], axis=0)\n",
    "          all_true_labels = torch.concat([all_true_labels, y], axis=0)\n",
    "\n",
    "          if log and all_true_labels.shape[0] % 500 == 0:\n",
    "              print(f\"{all_true_labels.shape[0]}, \", end='')\n",
    "\n",
    "    if log:\n",
    "      print(\"fin!\")\n",
    "    model.train()\n",
    "    return accuracy(all_output, all_true_labels, topk=topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4265bcf-3e77-4974-b2d3-502ca79aef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dgk27/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded resnet18!\n",
      " - Accuracy: (eval) 69.76%\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.31 GiB (GPU 0; 10.57 GiB total capacity; 4.63 GiB already allocated; 2.31 GiB free; 8.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# model.train() # TODO: see if makes a difference??\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# max_eval, min_eval, hvps, pos_evals, neg_evals, pos_bases = min_max_fn(\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     model, trainloader, criterion, use_cuda=True, verbose=False, **kwargs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# eigs = pos_evals.cpu().numpy()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# train_effective_dimension = eff_dim(eigs)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 36\u001b[0m max_eval, min_eval, hvps, pos_evals, neg_evals, pos_bases \u001b[38;5;241m=\u001b[39m min_max_fn(\n\u001b[1;32m     37\u001b[0m     model, testloader, criterion, use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neg_evals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     neg_evals \u001b[38;5;241m=\u001b[39m neg_evals\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/local/scratch/dgk27/degree-of-freedom/hess_vec_prod.py:67\u001b[0m, in \u001b[0;36mmin_max_hessian_eigs\u001b[0;34m(net, dataloader, criterion, rank, use_cuda, verbose, nsteps, return_evecs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# use lanczos to get the t and q matrices out\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m pos_q_mat, pos_t_mat \u001b[38;5;241m=\u001b[39m lanczos_tridiag(\n\u001b[1;32m     68\u001b[0m     hess_vec_prod,\n\u001b[1;32m     69\u001b[0m     nsteps,\n\u001b[1;32m     70\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     71\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     72\u001b[0m     matrix_shape\u001b[38;5;241m=\u001b[39m(N, N),\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# convert the tridiagonal t matrix to the eigenvalues\u001b[39;00m\n\u001b[1;32m     75\u001b[0m pos_eigvals, pos_eigvecs \u001b[38;5;241m=\u001b[39m lanczos_tridiag_to_diag(pos_t_mat)\n",
      "File \u001b[0;32m/local/scratch/dgk27/anaconda3/lib/python3.11/site-packages/linear_operator/utils/lanczos.py:117\u001b[0m, in \u001b[0;36mlanczos_tridiag\u001b[0;34m(matmul_closure, max_iter, dtype, device, matrix_shape, batch_shape, init_vecs, num_init_vecs, tol)\u001b[0m\n\u001b[1;32m    115\u001b[0m r_vec\u001b[38;5;241m.\u001b[39msub_(alpha_curr\u001b[38;5;241m.\u001b[39mmul(q_curr_vec))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Full reorthogonalization: r <- r - Q (Q^T r)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m correction \u001b[38;5;241m=\u001b[39m r_vec\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmul(q_mat[: k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39msum(dim_dimension, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m correction \u001b[38;5;241m=\u001b[39m q_mat[: k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmul(correction)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    119\u001b[0m r_vec\u001b[38;5;241m.\u001b[39msub_(correction)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.31 GiB (GPU 0; 10.57 GiB total capacity; 4.63 GiB already allocated; 2.31 GiB free; 8.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model_names = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
    "results = {\n",
    "    'params': {\n",
    "        'models': model_names,\n",
    "    },\n",
    "    'results': {},\n",
    "}\n",
    "\n",
    "min_max_fn = min_max_hessian_eigs\n",
    "kwargs = {\"nsteps\": 100} # default is 100\n",
    "START_INDEX = 0\n",
    "\n",
    "for MODEL_CHOICE in np.arange(START_INDEX, len(model_names)):\n",
    "\n",
    "    model = torch.hub.load('pytorch/vision', model_names[MODEL_CHOICE], weights=\"IMAGENET1K_V1\").to(device)\n",
    "    model.eval()\n",
    "    print(f\"Loaded {model_names[MODEL_CHOICE]}!\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #train_acc = evaluate_accuracy(model, trainloader, (1,))[0][1]\n",
    "    #print(f\" - Accuracy: (train) {train_acc:.2f}%\")\n",
    "    eval_acc = evaluate_accuracy(model, testloader, (1,))[0][1]\n",
    "    print(f\" - Accuracy: (eval) {eval_acc:.2f}%\")\n",
    "\n",
    "    # model.train() # TODO: see if makes a difference??\n",
    "    # max_eval, min_eval, hvps, pos_evals, neg_evals, pos_bases = min_max_fn(\n",
    "    #     model, trainloader, criterion, use_cuda=True, verbose=False, **kwargs\n",
    "    # )\n",
    "    # if neg_evals is not None:\n",
    "    #     neg_evals = neg_evals.cpu().numpy()\n",
    "    # eigs = pos_evals.cpu().numpy()\n",
    "    # train_effective_dimension = eff_dim(eigs)\n",
    "\n",
    "    model.eval()\n",
    "    max_eval, min_eval, hvps, pos_evals, neg_evals, pos_bases = min_max_fn(\n",
    "        model, testloader, criterion, use_cuda=True, verbose=False, **kwargs\n",
    "    )\n",
    "    if neg_evals is not None:\n",
    "        neg_evals = neg_evals.cpu().numpy()\n",
    "    eigs = pos_evals.cpu().numpy()\n",
    "    test_effective_dimension = eff_dim(eigs)\n",
    "\n",
    "    results['results'][model_names[MODEL_CHOICE]] = {\n",
    "        #'train_acc': train_acc,\n",
    "        'eval_acc': eval_acc,\n",
    "        # 'eff_dim_train': train_effective_dimension,\n",
    "        'eff_dim_test': test_effective_dimension,\n",
    "    }\n",
    "    print(results['results'][model_names[MODEL_CHOICE]])\n",
    "    torch.save(results, f'./resnet_eff_dims.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183e031-6545-49d0-aeae-ff18cfbadd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
